# Hack The Summer
## AIMLC, IIT Delhi

## Optical character recognition for simple formulas

My submission includes an AI training program, train.py, that can train an AI with decent accuracy without any external datasets, pretrained AI or human intervention, you can just give the path of the dataset and the annotations file and it will train the AI for you. It can be any dataset of the format in the problem statement given that it's big enough, it can even be in an entirely different language provided that the symbols are distinct enough. In fact you can try it out by downloading train.py and running python3 train.py /path/to/data /path/to/annotations/file.csv and it'll train an AI with about 97% accuracy in 20 to 70 minutes depending upon your hardware.

## Methodology used

At first sight it looks nearly impossible to train an AI just by the results of the formulas with just an MNIST-like AI (k-means clustering or something similar might work but I'm constraining myself to only what we have been taught in lectures). However upon further inspection you'll start noticing that there is some information you can know for certain from the data given to you. The trick is to look for the results that can be made in one way only and then using them to train an AI that can recognize those digits, which you can then use to recognize new digits. This goes on in a loop till you have datasets for every digit. I'll be explaining exactly what I did later on but first we need to make sure that it's easy for the AI to actually recognize digits, which is where image preprocessing and convolutions come in.

### Image Preprocessing

Recognizing symbols is hard for a machine, but what makes it even harder is unnecessary variations in data that only confuse the AI. Namely the position and size of the individual symbols. These attributes don't contribute anything to the recognition of the numbers and only cause inefficiencies, so our first step should be to eliminate them. I have implemented a simple function, cropAndProcessImage that takes an image, divides it into three parts, crops each part individually such that the digit is in the center and is scaled properly and then resizes them to 28 by 28 pixels (yes the size is inspired by the MNIST dataset). The reduction in size makes it so that the AI can easily process the data and counterintuitively, it doesn't even cause a huge loss in the overall "quality" of the image. This is because the given images are completely black and white with no grays, so converting them into images with a 0-255 pixel color variation with lower size doesn't lead to much difference. You can think of it as the edges being marked by gray colors. An example of the image preprocessing is given below.

 <img src="https://github.com/taitaisama/SOML-Hack-The-Summer/blob/main/images/1.jpg" alt="drawing" width="200"/>
 
 gets converted to
  <img src="https://github.com/taitaisama/SOML-Hack-The-Summer/blob/main/images/1_1.jpg" alt="drawing" width="200"/> 
  <img src="https://github.com/taitaisama/SOML-Hack-The-Summer/blob/main/images/1_2.jpg" alt="drawing" width="200"/> 
  <img src="https://github.com/taitaisama/SOML-Hack-The-Summer/blob/main/images/1_3.jpg" alt="drawing" width="200"/>

### Convolution using conv2d

Convolutions are used to highlight features of an input image that might not be obvious to humans but AIs can easily extract information out of them. The implementation in pytorch has in_channels, out_channels and kernel_size as input, it takes in_channels number of input tensors and outputs out_channels number of output tensors using kernels with random weights and biases. The convolutions are random but can be reproduced by using torch.manual_seed(seed) which makes it so that the same convolutions are generated every time. I have used two convolutions, one from 1 to 32, other from 32 to 64 and then a max_pool2d() which takes the largest of 4 bits of adjacent data, reducing the size by 4 times.

### Getting the first dataset 

As I've stated above as well, I used the fact that some results that can only be made in one way to get an initial set of data for some digits and operators. 81, for example, can only be made by 9 x 9 and similarly 64, 49 and 25 correspond to 8, 7 and 5 square. So using the data for infix, prefix and postfix we can get a dataset made for these numbers and also the multiplication operator. Other than this we also have 0 - 9 = -9 which gives us 0 dataset, 13 and 17 can only be made by addition because they are primes so they give us the addition dataset and all negative numbers give us the minus dataset. I process these results in processAnnotations() and then stored their corresponding processed images in separate directories for each number and operator.

### Getting the second dataset

Now that we have an initial set of data that we are sure of, we can train an AI in this data so that it can easily recognize 0, 5, 7, 8, 9, +, - and x. However the data isn't that much, as low as 150 images in the case of 0, so we can't really create a very accurate AI. An interesting observation is that training the AI for many iterations is actually decremental to its performance because the small dataset size makes it easily overfit and so I got the best results with a comparatively smaller number of iterations. For training I have used a generalised method train() which takes two lists and trains the digits from one list to map to the other list. So for example if I have the lists as [0, 1, 2, 3, 4, 5] and [0, 1, 3, 3, 3, 3] then it'll learn to recognize 0s and 1s as 0s and 1s but will learn 2, 3, 4, 5 as 3. So now that we have a trained AI we can use this AI to get a set of other digits. I do this by looking at multiples that require multiplication of one of the known digits and one of the unknown digits. So we have 27 which can only be made by 3 x 9, we know where the operator is so now we just have to determine which of the two digits is 3 and 9. The 9 will likely give us a 9 when put into the AI while the 3 can give us anything, so we assume that the digit that resulted in a random prediction to be 3 and store it. Similarly I've done the same thing for 4 x 7 = 28, 6 x 7 = 42, 4 x 5 = 20, 4 x 8 = 32 and 3 x 7 = 21. You might have noticed that I've missed 6 x 8 = 42, this is because 6 and 8 look very similar so the computer can't differentiate them well, resulting in many errors. With the aforementioned method the error rate is maximum for 3, with 14 errors out of 340ish results, which is acceptable for now (I calculated the errors with the AI that was finally made at the end). All of this happens in processAnnotations2().

### Getting division dataset

So after all of that we have a dataset for 0, 3, 4, 5, 6, 7, 8, 9, +, - and x with only /, 1 and 2 missing. First we make the division dataset. To do this I trained an AI with the lists [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] and [13, 13, 13, 13, 13, 13, 13, 13, 10, 11, 12]. Here 10 11 and 12 correspond to +, - and x respectively. You might be wondering why there are so many 13s in the second list, the reason for that is that this AI is specifically trained for operators only and because we don't know how division looks we have ¡°biased¡± the AI in such a way that if we are given anything random it'll give a 13 as a result. If this wasn't done and the AI was just trained normally then there would be a lot of errors as division looks most like subtraction. The way I have set it up makes it so that the AI has a pretty high chance of returning 13 given any random input that isn't +, - or x. Now we train another AI with both the lists as [5, 6, 7, 8, 9], so just training them to recognize these digits. I've just chosen these digits because my dataset for them is the most robust with 0 errors. Then we send both of these AIs to processDivision function which tries to look for cases where the result is 1 and the operator isn't +, - or x. So basically a/a cases. There's also a second layer of filtering with the other AI which checks if both the numbers give the same output, and if that's the case then the operator is likely to be division, so we store it as well.

### Getting ones and twos

One and Two are the only numbers left, so the function processOnesAndTwos deals with them. For this we train an AI with [10, 11, 12, 13] and [10, 11, 12, 13], so basically it can differentiate between operators very well. This AI gets sent along with the [5, 6, 7, 8, 9] AI and we look for the result to be 18, which can be made by 9 + 9 and 2 x 9. Then we check if both of the digits are 9 or if only one is nine and the other is something else, and if it's the latter we can also check if the operator is x or +. According to these results we can store the images of 2. For 1 we check for the results as 5 or 7, one if the digits being 5 or 7 and the operator being x, so this also has two layers of filtering. We chose 5 and 7 because they are primes so they can only be made with 1 if the operator is x. So after this function finishes we have a dataset of all the numbers and operators.

### Getting more from less

We have examples for every number, but we are still missing out on lots of potential data, so to get all of the data we can we call the makeAnnotations function with 4 models, one for operators and 3 for digits. I reused the previous operator AI but made 3 new digit AIs with both the lists as [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. These AIs also have some internal parameters changed from each other such as number of nodes in the hidden layer and convolution parameters. This is so that these AI's get trained differently and develop their own biases that are different from each other. Now we send these three AI's along with the operator one to makeAnnotations function and they all work together to try and recognize each digit in a given image. If all three of the digit AI's get the same answer and the final result matches the result from the annotations it is very likely that they predicted it correctly, so we label the data accordingly. Otherwise if one of the three gets a different answer or if the result doesn't match we move on without labeling. At the end we get 45902 labeled data, and when I checked for errors it had only 54. Upon analysing the errors it was evident that most of them came from 0 x a, where a gets wrongly predicted by all three AIs, but the result is still the same, 0, so we it gets labeled. To mitigate this I filtered out all images that were predicted as 0 x a or 0 / a type along with some other problematic formulas in a function getInvalidCondition. This decreases the number of images to around 39000 but the errors also get reduced to only 2. One is an image of 4 / 2 with the dots of the / being very small so it gets recognized as a -, and the other one is 20117.jpg, an 8 / 8 image with really weird looking 8s so they both get predicted as 7.

### Training the final AI

Now that we have a pretty big dataset of 39000 images we can train our final AI that gets trained in all [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. I train it in 5 different epochs, but I haven't used the DataLoader class because the data I have is very inconsistently sized, with like 10000 minuses and just 1500 division symbols. To make it not biased I created another function that randomly chooses a number from 0 to 13 and gives an image from the corresponding directories of these numbers/operators. After training the AI gets an accuracy of 99% on individual symbols, 1604/150000 wrong to be exact, which would be 97% if you are calculating for three symbols. Admittedly the AI I am using in the competition hasn't exactly been made by this process, in that one I used the final AI to eliminate errors from previously created datasets and also used another AI from another dataset (the previously given one, not external) just to have some variance. I also manually changed some annotations made by the AI if they were wrong, so like the 54 errors I was getting previously. By doing these thing I got an AI with better accuracy, but this process required some human intervention so isn't as elegant in my opinion. I have included the 97% AI in the zipped file as well and you can also give the path to the dataset and annotations file to train.py and generate it yourself.

I realisize the AI isn't as optimised as it can be, but I have put in a lot of effort to make it as good as it already is. There are many functions that I have commented out and many others that I just deleted. My orignal plan was to train 3 AIs similar to the final 97% accuracy and then re-annotate the data using them (makeAnnotations2 and 3 are for this purpose), but the improvements werent drastic and they took a lot of time so I ended up going with a simpler version. Further plans I would have liked to implement were k-means clustering along with regression to reach better accuracy, but time limitations didn't allow me to do that. Hope you like whatever I was able to make though.
